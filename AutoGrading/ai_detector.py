from pytesseract import Output
import io
from pathlib import Path
from docx import Document
from PIL import Image
import pytesseract
import os
from transformers import AutoModelForSequenceClassification, AutoTokenizer, logging
import torch
import json
from math import ceil, floor

logging.set_verbosity_error()
# ========== CONFIG ==========
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
os.environ['TESSDATA_PREFIX'] = r"C:\Program Files\Tesseract-OCR\tessdata"
FOLDER_PATH = r"C:\Documents\Code\UtilityBox\AutoGrading\Tuan 6"
THRESHOLD_POINT_CHECK_AI = 0.2
JSON_PATH = r"C:\Documents\Code\UtilityBox\AutoGrading\output.json"
# ========== HÃ€M ==========
file_path = r"C:\Documents\Code\UtilityBox\AutoGrading\Tuan 6\2400113059_LamTaiChanh_Lab6.docx"


def ocr_preserve_layout(img_data, lang="eng+vie", psm=6):
    try:
        # Má»Ÿ áº£nh tá»« bytes
        img = Image.open(io.BytesIO(img_data)).convert("RGB")

        # Láº¥y dá»¯ liá»‡u chi tiáº¿t tá»«ng tá»«
        data = pytesseract.image_to_data(
            img,
            lang=lang,
            config=f"--psm {psm}",
            output_type=Output.DICT
        )

        # Gom text theo block vÃ  line
        blocks = {}
        for i, text in enumerate(data["text"]):
            if text.strip():
                block = data["block_num"][i]
                line = data["line_num"][i]
                blocks.setdefault(block, {})
                blocks[block].setdefault(line, [])
                blocks[block][line].append(text)

        # Gá»™p thÃ nh text, cÃ³ cÃ¡ch dÃ²ng giá»¯a cÃ¡c block
        text = "\n\n".join(
            "\n".join(" ".join(blocks[b][l]) for l in sorted(blocks[b]))
            for b in sorted(blocks)
        )

        return text.strip()

    except Exception as e:
        print(f"Lá»—i OCR: {e}")
        return ""


def extract_text_from_images_in_docx():
    folder_path = Path(FOLDER_PATH)
    data = []
    for doc_file in folder_path.glob("*.docx"):
        doc = Document(doc_file)
        rels = doc.part.rels
        all_text_image = []

        all_text = [p.text.strip() for p in doc.paragraphs if p.text.strip()]
        text_output = "\n".join(all_text)

        for rel in rels.values():
            if "image" in rel.target_ref:
                try:
                    img_data = rel.target_part.blob

                    # OCR áº£nh (tá»± Ä‘á»™ng nháº­n tiáº¿ng Anh + Viá»‡t)
                    text = ocr_preserve_layout(
                        img_data, lang="eng+vie", psm=6)

                    all_text_image.append(text)
                except Exception as e:
                    print(f"Lá»—i Ä‘á»c áº£nh: {e}")
                    continue

        data.append({
            "file_name": doc_file.stem.strip(),
            "text_image":  "\n\n".join(all_text_image),
            "text": text_output
        })

    return data


model_name = "roberta-base-openai-detector"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)


def detect_ai_text(text):
    """Tráº£ vá» xÃ¡c suáº¥t vÄƒn báº£n lÃ  do AI viáº¿t (0â€“1)."""
    inputs = tokenizer(text, return_tensors="pt", truncation=True)
    outputs = model(**inputs)
    probs = torch.softmax(outputs.logits, dim=1)
    ai_score = probs[0][1].item()
    return ai_score


def classify_level_use_ai(ai_score, minus):
    level = ""
    response = ""
    if ai_score >= 15:
        level = "BÃ i lÃ m gáº§n nhÆ° do AI táº¡o ra hoÃ n toÃ n"
        response = f"Kháº£ nÄƒng cá»±c cao bÃ i nÃ y Ä‘Æ°á»£c sinh ra hoÃ n toÃ n bá»Ÿi AI. Háº§u nhÆ° khÃ´ng cÃ³ dáº¥u hiá»‡u can thiá»‡p thá»§ cÃ´ng. BÃ i cá»§a báº¡n sáº½ bá»‹ trá»« {minus}Ä‘"
    elif ai_score >= 10:
        level = "Phá»¥ thuá»™c nhiá»u vÃ o AI"
        response = f"Pháº§n lá»›n ná»™i dung cÃ³ dáº¥u hiá»‡u Ä‘Æ°á»£c sinh ra bá»Ÿi AI. BÃ i lÃ m thiáº¿u dáº¥u áº¥n cÃ¡ nhÃ¢n, tÆ° duy riÃªng vÃ  sai khÃ¡c vá»›i cÃ¡ch diá»…n Ä‘áº¡t cá»§a sinh viÃªn. BÃ i cá»§a báº¡n sáº½ bá»‹ trá»« {minus}Ä‘"
    elif ai_score >= 5:
        level = "Kháº£ nÄƒng cao sá»­ dá»¥ng AI"
        response = f"BÃ i lÃ m cÃ³ nhiá»u pháº§n trÃ¹ng khá»›p vá»›i Ä‘áº·c trÆ°ng cá»§a vÄƒn báº£n sinh bá»Ÿi AI. Nhiá»u cÃ¢u, Ä‘oáº¡n vÄƒn thá»ƒ hiá»‡n Ä‘á»™ mÆ°á»£t báº¥t thÆ°á»ng. BÃ i cá»§a báº¡n sáº½ bá»‹ trá»« {minus}Ä‘"
    elif ai_score > 1:
        level = "CÃ³ dáº¥u hiá»‡u AI nhÆ°ng trong giá»›i háº¡n cho phÃ©p"
        response = "BÃ i lÃ m cÃ³ dáº¥u hiá»‡u AI nháº¹, nhÆ°ng váº«n trong má»©c cho phÃ©p. CÃ¡ch diá»…n Ä‘áº¡t vÃ  cáº¥u trÃºc ná»™i dung khÃ¡ trÃ´i cháº£y, tá»± nhiÃªn. Báº¡n sáº½ khÃ´ng bá»‹ trá»« Ä‘iá»ƒm vÃ¬ yáº¿u tá»‘ nÃ y."
    else:
        level = "KhÃ´ng phÃ¡t hiá»‡n dáº¥u hiá»‡u AI"
        response = "BÃ i lÃ m thá»ƒ hiá»‡n sá»± trung thá»±c vÃ  ná»— lá»±c rÃµ rá»‡t cá»§a sinh viÃªn. CÃ¡ch diá»…n Ä‘áº¡t tá»± nhiÃªn, máº¡ch láº¡c vÃ  cÃ³ tÆ° duy riÃªng. Ráº¥t Ä‘Ã¡ng khen vÃ¬ em khÃ´ng phá»¥ thuá»™c vÃ o AI."

    return level, response


def analyze_submissions():
    results = []
    for text in extract_text_from_images_in_docx():
        text_image = detect_ai_text(text['text_image'])
        text_normal = detect_ai_text(text['text'])
        ai_score = round((text_normal + text_image) * 10, 3)
        minus = round(ai_score * THRESHOLD_POINT_CHECK_AI, 3)
        level, response = classify_level_use_ai(ai_score, minus)
        results.append({
            "file_name": text["file_name"],
            "ai_score": ai_score,
            "minus": minus,
            "level": level,
            "response": response
        })

    return results


def update_scores_after_ai(output):
    with open(JSON_PATH, "r", encoding="UTF-8") as f:
        data_json = json.load(f)
    for r in output:
        print(
            f"ðŸ“„ {r['file_name']}: AI score = {r['ai_score']} -> Sáº½ bá»‹ trá»«: {r['minus']}Ä‘")
        for obj in data_json:
            name = obj.get("name", "")
            if (name == r['file_name']):
                point = float(
                    obj.get("total_point", 0)) - float(r.get("minus", 0))
                if (int(point) - point >= 0.5):
                    obj['total_point'] = ceil(point)
                elif (int(point) - point < 0.5):
                    obj['total_point'] = floor(point)
                break

    with open(JSON_PATH, "w", encoding="UTF-8") as f:
        json.dump(data_json, f, ensure_ascii=False, indent=4)
    print("âœ… ÄÃ£ cáº­p nháº­t vÃ  lÆ°u file JSON thÃ nh cÃ´ng!")


if __name__ == "__main__":
    output = analyze_submissions()
    update_scores_after_ai(output)
