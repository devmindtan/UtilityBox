import re
from docx import Document
from bs4 import BeautifulSoup
from google import genai
from google.genai import types
from pathlib import Path
import json
import pandas as pd
import filetype
import fitz
import os
from dotenv import load_dotenv

load_dotenv()

FOLDER_PATH = r"C:\Documents\Code\UtilityBox\AutoGrading\Tuan 6"
TOPIC_FILE = "Lab2.CacWidgetCoBan.docx"
API_KEY = os.getenv('API_KEY')
MODEL_NAME = "gemini-2.5-flash"
# RUBRIC = """
# Rubric ch·∫•m ƒëi·ªÉm (t·ªïng 10 ƒëi·ªÉm):
# 1. N·ªôp code (4ƒë)
# 2. Gi·∫£i th√≠ch √Ω t∆∞·ªüng (4ƒë) (Gi·∫£i th√≠ch √Ω t∆∞·ªüng c·ªßa m√¨nh tr∆∞·ªõc khi code kh√¥ng gi·∫£i th√≠ch nh·ªØng d√≤ng code)
#     - Ng·∫Øn g·ªçn x√∫c t√≠ch
#     - Th·ªÉ hi·ªán m√¨nh hi·ªÉu ƒë∆∞·ª£c b√†i t·∫≠p
#     - ƒê√°nh gi√° cao nh·ªØng √Ω t∆∞·ªüng c√° nh√¢n kh√¥ng theo l·ªëi m√≤n
#     => N·∫øu kh√¥ng l√†m ƒë∆∞·ª£c nh·ªØng c√°i tr√™n s·∫Ω b·ªã tr·ª´ ƒëi·ªÉm
# 3. C√≥ k·∫øt ch·∫°y ƒë∆∞·ª£c (1ƒë)
# 4. Tr√¨nh b√†y g·ªçn g√†ng ƒë·∫ßy ƒë·ªß (1ƒë)
# ƒê·∫∑c bi·ªát: n·∫øu t√™n file kh√¥ng ƒë√∫ng theo quy chu·∫©n n√†y HoTen_MSSV_lab6 (V√≠ d·ª•: NguyenVanA_22133422_lab6) s·∫Ω tr·ª´ 1 ƒëi·ªÉm
# """

RUBRIC = """
Rubric ch·∫•m ƒëi·ªÉm (t·ªïng 10 ƒëi·ªÉm):
B√†i t·∫≠p l·∫ßn n√†y kh√¥ng y√™u c·∫ßu g√¨ nhi·ªÅu ch·ªâ c·∫ßn code theo xong th√¨ gi·∫£i th√≠ch nh·ªØng g√¨ b·∫°n hi·ªÉu trong ƒëo·∫°n code m·∫´u.
1. Code (7ƒë)
2. Gi·∫£i th√≠ch (3ƒë)
L∆∞u √Ω: gi·∫£i th√≠ch b·∫±ng ch√≠nh vƒÉn c·ªßa m√¨nh, kh√¥ng ƒë·∫°o, kh√¥ng copy, hi·ªÉu g√¨ n√≥i ƒë√≥, n·∫øu vi ph·∫°m th√¨ s·∫Ω b·ªã tr·ª´ ƒëi·ªÉm
"""


# ========== INIT CLIENT ==========
client = genai.Client(api_key=API_KEY)


# X√°c ƒë·ªãnh ki·ªÉu c·ªßa ·∫£nh
def detect_mine_type(image_data):
    mime_type = ""

    kind = filetype.guess(image_data)
    if kind:
        mime_type = kind.mime
    else:
        mime_type = "image/png"

    return mime_type


# X·ª≠ l√≠ ƒë·∫ßu v√†o
def safe_json_loads(text):
    # C·∫Øt b·ªè m·ªçi th·ª© ngo√†i JSON th·∫≠t
    match = re.search(r"\{.*\}", text, re.DOTALL)
    if match:
        clean = match.group(0).strip()
        return json.loads(clean)
    else:
        raise ValueError("Kh√¥ng t√¨m th·∫•y JSON h·ª£p l·ªá trong ph·∫£n h·ªìi")


def read_doc_content(max_files=None):
    folder_path = Path(FOLDER_PATH)
    data = []

    for i, doc_file in enumerate(folder_path.glob("*.docx"), start=1):
        if (max_files is not None and i > max_files):
            break

        doc = Document(doc_file)
        rels = doc.part.rels

        # ======= ƒê·ªçc text =======
        all_text = []
        for para in doc.paragraphs:
            all_text.append(para.text.strip())
        text_output = "\n".join(all_text)

        # ======= ƒê·ªçc ·∫£nh =======
        image_parts = []
        for rel in rels.values():
            if "image" in rel.target_ref:
                image_data = rel.target_part.blob
                image_parts.append(
                    types.Part.from_bytes(
                        data=image_data, mime_type=detect_mine_type(image_data))
                )

        data.append({
            "file_name": doc_file.stem.strip(),
            "text": text_output,
            "images": image_parts
        })

    return data


def read_pdf_content():
    folder_path = Path(FOLDER_PATH)
    data = []

    for pdf_file in folder_path.glob("*.pdf"):
        pdf = fitz.open(pdf_file)
        all_text = []
        image_parts = []

        for page in pdf:
            # L·∫•y text
            all_text.append(page.get_text())

            # L·∫•y ·∫£nh
            for img in page.get_images(full=True):
                xref = img[0]
                image_data = pdf.extract_image(xref)["image"]
                image_parts.append(types.Part.from_bytes(
                    data=image_data,
                    mime_type=detect_mine_type(image_data)
                ))

        text_output = "\n".join(all_text)

        data.append({
            "file_name": pdf_file.stem.strip(),
            "text": text_output,
            "images": image_parts
        })

    return data


def read_topic_content():
    path = Path(TOPIC_FILE)
    if not path.exists():
        raise FileNotFoundError(f"File ƒë·ªÅ b√†i kh√¥ng t·ªìn t·∫°i: {TOPIC_FILE}")

    # ===== HTML =====
    if path.suffix.lower() == ".html":
        with open(path, "r", encoding="utf-8") as f:
            html = f.read()
        soup = BeautifulSoup(html, "html.parser")
        return {"content": soup.get_text().strip(), "image_content": []}

    # ===== DOCX =====
    elif path.suffix.lower() == ".docx":
        doc = Document(path)
        content = "\n".join([p.text.strip()
                             for p in doc.paragraphs if p.text.strip()])
        rels = doc.part.rels.values()
        image_parts = []
        for rel in rels:
            if "image" in rel.target_ref:
                image_data = rel.target_part.blob
                image_parts.append(
                    types.Part.from_bytes(
                        data=image_data, mime_type=detect_mine_type(image_data))
                )
        return {"content": content, "image_content": image_parts}

    else:
        raise ValueError("File ƒë·ªÅ b√†i ph·∫£i l√† HTML ho·∫∑c DOCX")


def read_all_content():
    all_data = []

    # DOCX
    all_data.extend(read_doc_content())

    # PDF
    all_data.extend(read_pdf_content())

    return all_data


# ========== T·∫†O PROMPT ==========
def create_prompt(file_name, content, level, response):
    return f"""
    B·∫°n l√† gi·∫£ng vi√™n ƒë·∫°i h·ªçc ch·∫•m b√†i sinh vi√™n. H√£y ƒë·ªçc k·ªπ **ƒë·ªÅ b√†i**, **rubric**, v√† **n·ªôi dung b√†i l√†m** (text ho·∫∑c h√¨nh ·∫£nh).

    ## Nhi·ªám v·ª•:
    1. ƒê√°nh gi√° chi ti·∫øt t·ª´ng ti√™u ch√≠ trong rubric.
    2. Ghi r√µ ƒëi·ªÉm cho t·ª´ng ti√™u ch√≠.
    3. T√≠nh t·ªïng ƒëi·ªÉm (thang 10) (l√†m tr√≤n .5 tr·ªü l√™n l√† l√™n, d∆∞·ªõi .5 l√† xu·ªëng).
    4. Nh·∫≠n x√©t ng·∫Øn g·ªçn (d∆∞·ªõi 30 ch·ªØ/ti√™u ch√≠), chuy√™n nghi·ªáp, x∆∞ng "em".
    5. Khi vi·∫øt nh·∫≠n x√©t chi ti·∫øt v√† t·ªïng qu√°t, **tham kh·∫£o `response` trong `ai_review` ƒë·ªÉ ƒëi·ªÅu ch·ªânh c√°ch di·ªÖn ƒë·∫°t**, v√≠ d·ª•:
       - N·∫øu `response` nh·∫•n m·∫°nh b√†i v·∫´n trong gi·ªõi h·∫°n cho ph√©p, h√£y d√πng l·ªùi kh√≠ch l·ªá, nh·∫π nh√†ng.
       - N·∫øu `response` nh·∫•n m·∫°nh b√†i c√≥ d·∫•u hi·ªáu AI r√µ r·ªát, h√£y th·ªÉ hi·ªán s·ª± nh·∫Øc nh·ªü/nh·∫•n m·∫°nh nh∆∞ng v·∫´n trung th·ª±c, chuy√™n nghi·ªáp.
    6. Tr·∫£ v·ªÅ **duy nh·∫•t** m·ªôt JSON h·ª£p l·ªá theo m·∫´u sau:

    ---
    {{
      "name": "{file_name}",
      "total_point": <s·ªë ƒëi·ªÉm tr√™n 10>,
      "detail": {{
        "T√™n ti√™u ch√≠ 1": "[ƒêi·ªÉm] ƒëi·ªÉm ‚Äî [Nh·∫≠n x√©t ng·∫Øn d·ª±a tr√™n response]",
        "T√™n ti√™u ch√≠ 2": "[ƒêi·ªÉm] ƒëi·ªÉm ‚Äî [Nh·∫≠n x√©t ng·∫Øn d·ª±a tr√™n response]",
        ...
      }},
      "general": "<nh·∫≠n x√©t t·ªïng qu√°t 1‚Äì2 c√¢u d·ª±a tr√™n response>",
      "ai_review": {{
         "muc_do": {level},
         "phan_hoi": "{response}"
      }}
    }}
    ---

    ## Th√¥ng tin ch·∫•m:
    **ƒê·ªÅ b√†i:**
    {read_topic_content()}

    **Rubric:**
    {RUBRIC}

    **N·ªôi dung b√†i l√†m (text):**
    {content}

    N·∫øu c√≥ h√¨nh ·∫£nh, h√£y x√©t n·ªôi dung trong ·∫£nh n·ªØa.
    """


# ========== G·ªåI GEMINI ==========
def genemi_call(prompt_text, images):
    response = client.models.generate_content(
        model=MODEL_NAME,
        contents=[
            types.Part.from_text(text=prompt_text),
            *images
        ]
    )

    return response.text


# ========== CH·∫§M ƒêI·ªÇM H√ÄNG LO·∫†T ==========
def grading():
    all_res = []
    for doc in read_all_content():
        print(f"üìÑ {doc["file_name"]} ‚Äî ƒê√£ ƒë·ªçc {len(doc["images"])} ·∫£nh")
        file_name = doc["file_name"]
        content = doc["text"]
        images = doc["images"]
        prompt_text = create_prompt(file_name, content)
        res = genemi_call(prompt_text, images)
        all_res.append(safe_json_loads(res))

    with open("output.json", "w", encoding="UTF-8") as f:
        json.dump(all_res, f, ensure_ascii=False, indent=4)


# ========== T·∫†O FILE B√ÅO C√ÅO EXEL ==========
def create_excel_report(json_file="output.json", excel_file="output.xlsx"):
    sheet_name = "K·∫øt qu·∫£ ch·∫•m ƒëi·ªÉm"

    # ƒê·ªçc JSON
    with open(json_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    # Chuy·ªÉn dict 'detail' th√†nh string ƒë·ªÉ gi·ªØ trong 1 c·ªôt
    for d in data:
        d['detail'] = "\n".join(
            [f"‚Ä¢ {k}: {v}" for k, v in d['detail'].items()])

    #  T·∫°o DataFrame
    # Ch·ª©a t·∫•t c·∫£ c√°c c·ªôt: name, total_point, detail, general
    df = pd.DataFrame(data)

    # Excel writer v·ªõi xlsxwriter
    writer = pd.ExcelWriter(excel_file, engine='xlsxwriter')
    df.to_excel(writer, index=False, sheet_name=sheet_name)

    workbook = writer.book
    worksheet = writer.sheets[sheet_name]

    # Format wrap text + middle align
    cell_format_1 = workbook.add_format({
        'align': 'center',
        'valign': 'vcenter',
        'text_wrap': True
    })
    cell_format_2 = workbook.add_format({
        'valign': 'vcenter',
        'text_wrap': True
    })

    # √Åp d·ª•ng format + auto column width
    for i, col in enumerate(df.columns):
        max_len = max(df[col].astype(str).map(len).max(), len(col)) + 2
        max_len = min(max_len, 80)  # gi·ªõi h·∫°n max width 80

        if i < 2:  # ch·ªâ 2 c·ªôt ƒë·∫ßu
            worksheet.set_column(i, i, max_len, cell_format_1)
        else:  # c√°c c·ªôt c√≤n l·∫°i
            worksheet.set_column(i, i, max_len, cell_format_2)

    # L∆∞u file
    writer.close()
    print("‚úÖ ƒê√£ t·∫°o file output.xlsx")


if __name__ == "__main__":
    grading()
    create_excel_report()
    # print(read_topic_content())
